{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/qg-bert-en-dec/model_4\n",
      "/kaggle/input/qg-bert-en-dec/model_3\n",
      "/kaggle/input/qg-bert-en-dec/model_1\n",
      "/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n",
      "/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertConfig\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "\n",
    "def get_tokenizer(model_type_path):\n",
    "    tok = BertTokenizer.from_pretrained(model_type_path, do_lower_case=True)\n",
    "    return tok\n",
    "\n",
    "def get_mask_ids(tokens, max_seq_length):\n",
    "    \"\"\"attention Mask id for padding 1 for original 0 for padded\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segment_ids(tokens, max_seq_length):\n",
    "    \"\"\"Segments id : 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "                current_segment_id = 1\n",
    "    assert current_segment_id ==1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_token_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "\n",
    "def process_input(text, ans, ques, tokenizer, max_seq_len):\n",
    "    text_token = tokenizer.tokenize(text)\n",
    "    ans_token= tokenizer.tokenize(ans)\n",
    "    ques_token= tokenizer.tokenize(ques)\n",
    "\n",
    "    if len(text_token) > max_seq_len-3 -len(ans_token):\n",
    "        text_token = text_token[:max_seq_len- 3-len(ans_token)]\n",
    "\n",
    "    return text_token, ans_token, ques_token\n",
    "    \n",
    "def pad_ques(ques, max_seq_len, padding =0):\n",
    "    if len(ques)> max_seq_len:\n",
    "        raise IndexError(\"len of ques {} greater than max_seq_len{}\".format(len(ques), max_seq_len))\n",
    "\n",
    "    req_len= max_seq_len- len(ques)\n",
    "    ques += [padding]* req_len\n",
    "\n",
    "    return ques\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_input(text, ans, ques, tokenizer, max_seq_len):\n",
    "    text_token, ans_token, ques_token= process_input(text,ans,ques, tokenizer, max_seq_len)\n",
    "\n",
    "    allToken= [\"[CLS]\"] + text_token  + [\"[SEP]\"] + ans_token + [\"[SEP]\"]\n",
    "    ques_token= [\"[CLS]\"] + ques_token  + [\"[SEP]\"]\n",
    "    ids= get_token_ids(allToken, tokenizer, max_seq_len)\n",
    "    mask_ids = get_mask_ids(allToken, max_seq_len)\n",
    "    segment_ids = get_segment_ids(allToken, max_seq_len)\n",
    "\n",
    "    que_ids= get_token_ids(ques_token, tokenizer, len(ques_token))\n",
    "\n",
    "    que_ids= pad_ques(que_ids, max_seq_len) #CR padding\n",
    "    assert len(que_ids)== max_seq_len\n",
    "    return ids, mask_ids, segment_ids, que_ids\n",
    "\n",
    "\n",
    "def generate_context_list(text):\n",
    "    ''' Extract chunks with spacy from text paragraph\n",
    "    '''\n",
    "    nlp= spacy.load('en_core_web_sm') #this will download spacy english core model\n",
    "    nlpDoc= nlp(text)\n",
    "    lst= []\n",
    "    for chunk in nlpDoc.noun_chunks:\n",
    "        lst.append(chunk)\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QGDataset(Dataset):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args= args\n",
    "        self.examples= []\n",
    "        self.tokenizer= get_tokenizer(args.bert_model)\n",
    "        self.max_seq_len= self.args.max_seq_len\n",
    "\n",
    "        if args.squad_path== None and args.inferenceMode == False:\n",
    "            raise ValueError(\"invaild path to squad 1.0 data or wrong inference mode\")\n",
    "\n",
    "        if args.squad_path != None:\n",
    "            with open(args.squad_path) as f:\n",
    "                json_data= json.load(f)\n",
    "                json_data = json_data['data']\n",
    "\n",
    "            for data in json_data:\n",
    "                for para in data['paragraphs']:\n",
    "                    con = para['context']\n",
    "                    qas= para['qas']\n",
    "                    for xs in qas:\n",
    "                        cur_ans= xs['answers'][0]['text']\n",
    "                        cur_ans_offset= xs['answers'][0]['answer_start']\n",
    "                        que= xs['question']\n",
    "                        ex= {\n",
    "                            'text': con,\n",
    "                            'ans': cur_ans,\n",
    "                            'ans_offset': cur_ans_offset,\n",
    "                            'ques': que\n",
    "                        }\n",
    "                        self.examples.append(ex)\n",
    "            del json_data\n",
    "        \n",
    "        else:\n",
    "            if args.inferenceFile == None:\n",
    "                raise ValueError(\"wrong file for inference\")\n",
    "            \n",
    "            with open(args.inferenceFile) as f:\n",
    "                json_data= json.load(f)\n",
    "            \n",
    "            for item in json_data:\n",
    "                paragraph= None\n",
    "                contextList= None\n",
    "                if 'paragraph' in item.keys():\n",
    "                    paragraph = item['paragraph']\n",
    "                \n",
    "                else:\n",
    "                    raise KeyError(\"no text para graph is found. worong format of inference file\")\n",
    "                \n",
    "                if 'context_list' in  item.keys():\n",
    "                    contextList= item['context_list']\n",
    "                \n",
    "                else:\n",
    "                    contextList = generate_context_list(paragraph)\n",
    "                \n",
    "                for context in contextList:\n",
    "                    cur_ex= {\n",
    "                        'text': paragraph,\n",
    "                        'ans': context,\n",
    "                        'ques': \"a dummy ques to avoid None error\"\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(cur_ex)\n",
    "                \n",
    "\n",
    "        if args.occu:\n",
    "            self.examples= self.examples[:args.occu]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        cur_ex= self.examples[idx]\n",
    "        ids, mask_id, seg_id, ques= convert_to_input(cur_ex['text'], cur_ex['ans'], cur_ex['ques'], self.tokenizer, self.max_seq_len)\n",
    "\n",
    "        exm= {\n",
    "            'ids': torch.tensor(ids, dtype= torch.long),\n",
    "            'mask_ids': torch.tensor(mask_id, dtype= torch.long),\n",
    "            'segment_ids': torch.tensor(seg_id, dtype= torch.long),\n",
    "            'ques': torch.tensor(ques, dtype= torch.long)\n",
    "        }\n",
    "#         print(exm)\n",
    "        return exm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def get_blue_score(orig, pre):\n",
    "#     print(\"orignal blue ->>>>:\" , orig,\"\\n Predicted blue->>>>\", pre)\n",
    "    orig_tok= orig.split()\n",
    "    pre_tok= pre.split()[:len(orig_tok)]\n",
    "#     print(\"orignal blue ->>>>:\" , orig_tok,\"\\n Predicted blue->>>>\", pre_tok)\n",
    "    ref= [orig_tok]\n",
    "    score= sentence_bleu(ref, pre_tok)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# from arguments import inferenceArgs\n",
    "# from process import QGDataset\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    if torch.cuda.is_available():\n",
    "        device= torch.device(\"cuda\")  \n",
    "    else:\n",
    "        device= torch.device(\"cpu\")\n",
    "        print(\"using CPU as device, may cause Out of memory error\")\n",
    "\n",
    "    evalData= QGDataset(args)\n",
    "    batch_size = min(16,evalData.__len__())\n",
    "    ignore_label= -1 #check\n",
    "    worker=0\n",
    "    model= torch.load(args.infereceModelPath)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = evalData.tokenizer\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    evalDataLoader = DataLoader(evalData,batch_size=batch_size, num_workers= worker)\n",
    "    total_acc= AverageMeter()\n",
    "    tdl = tqdm(evalDataLoader, total= len(evalDataLoader))\n",
    "    for idx,batch in enumerate(tdl):\n",
    "    \n",
    "        ids = batch['ids'].to(device, dtype=torch.long)\n",
    "        mask_ids = batch['mask_ids'].to(device, dtype=torch.long)\n",
    "        seg_ids = batch['segment_ids'].to(device, dtype=torch.long)\n",
    "        ques = batch['ques'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits= model(\n",
    "                input_ids= ids,\n",
    "                attention_mask= mask_ids,\n",
    "                decoder_input_ids= ids,\n",
    "                # decoder_inputs_embeds= model.get_input_embeddings().weight,\n",
    "                token_type_ids= seg_ids,\n",
    "                masked_lm_labels = None\n",
    "            )\n",
    "#         print(logits)\n",
    "        logits= logits[0]\n",
    "        logits= logits.view(-1, vocab_size)\n",
    "        # orig_ques= ques.view(-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        pred_ques = np.argmax(logits, axis=1).flatten().squeeze()\n",
    "        pred_ques = np.reshape(pred_ques,(batch_size,-1))\n",
    "        for i in range(ids.shape[0]):\n",
    "            cur_pred_ques= list(pred_ques[i])\n",
    "            try:\n",
    "                cur_len= cur_pred_ques.index(102) # find first sep token\n",
    "            except ValueError:\n",
    "                cur_len= len(cur_pred_ques)-1\n",
    "\n",
    "            cur_pred_ques = cur_pred_ques[:cur_len+1]\n",
    "            cur_pred_ques= tokenizer.decode(cur_pred_ques, skip_special_tokens=True)\n",
    "            cur_orignal_ques= tokenizer.decode(list(ques[i]), skip_special_tokens=True)\n",
    "            ca= get_blue_score(cur_orignal_ques, cur_pred_ques)\n",
    "            total_acc.update(ca)\n",
    "            tdl.set_postfix(status= 'valid',accu= total_acc.avg)\n",
    "#             print(\"orignal ->>>>:\" , cur_orignal_ques,\"\\n Predicted->>>>\", cur_pred_ques)\n",
    "            \n",
    "#             print(cur_pred_ques)\n",
    "\n",
    "    print(\"avga accu: \", total_acc.avg)\n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     inference(inferenceArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b5ba1b5c16430ea12456c85812c90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertConfig \n",
    "\n",
    "bert_model= 'bert-base-uncased'\n",
    "max_seq_len =256\n",
    "\n",
    "class trainDataArgs:\n",
    "    bert_model= bert_model\n",
    "    max_seq_len = max_seq_len\n",
    "    squad_path= '../input/stanford-question-answering-dataset/train-v1.1.json' #CR\n",
    "    inferenceMode= False  # T/F for inference\n",
    "    inferenceFile= None  # if nferenceMode = True: a path to json file consisting of list of dict with keys 'paragraph' as mandatory key and 'context_list' as option  [{'paragraph':' a string', 'context_list': ['list' , 'of', 'context', 'i.e answers'] }, {}, . . .]\n",
    "\n",
    "    occu = 30000\n",
    "\n",
    "class validDataArgs:\n",
    "    bert_model= bert_model\n",
    "    max_seq_len =max_seq_len\n",
    "    squad_path= '../input/stanford-question-answering-dataset/dev-v1.1.json' #CR\n",
    "    inferenceMode= False  # T/F for inference\n",
    "    inferenceFile= None  # if nferenceMode = True: a path to json file consisting of list of dict with keys 'paragraph' as mandatory key and 'context_list' as option  [{'paragraph':' a string', 'context_list': ['list' , 'of', 'context', 'i.e answers'] }, {}, . . .]\n",
    "    occu= 8000\n",
    "\n",
    "class inferenceArgs:\n",
    "    bert_model= bert_model\n",
    "    max_seq_len =max_seq_len\n",
    "    squad_path= '/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json' #CR\n",
    "    inferenceMode= False  # T/F for inference\n",
    "    inferenceFile= None  # CR\n",
    "    # if nferenceMode = True: a path to json file consisting of list of dict with keys 'paragraph' as mandatory key and 'context_list' as option  [{'paragraph':' a string', 'context_list': ['list' , 'of', 'context', 'i.e answers'] }, {}, . . .]\n",
    "    occu= None\n",
    "    infereceModelPath= '/kaggle/input/qg-bert-en-dec/model_4'\n",
    "\n",
    "\n",
    "class trainingConfig:\n",
    "    bert_model = bert_model\n",
    "    bert_config = BertConfig.from_pretrained(bert_model)\n",
    "    max_seq_len= max_seq_len\n",
    "    train_batch_size= 16\n",
    "    valid_batch_size= 16\n",
    "    ignore_label =0\n",
    "    num_workers=0\n",
    "    epochs =3\n",
    "    learningRate = 5e-5\n",
    "    save_dir = '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca46eb8edaca4c23b995acd20b361703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/661 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "  0%|          | 1/661 [00:01<21:07,  1.92s/it, accu=0.64, status=valid]/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "  2%|▏         | 10/661 [00:12<12:17,  1.13s/it, accu=0.471, status=valid]/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 661/661 [12:14<00:00,  1.11s/it, accu=0.369, status=valid]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avga accu:  0.3691226051927911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference(inferenceArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "013a28c3c6f24892a529d8e14ce0790f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5439e06ef23d45ac8613c56488a460ea",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_90b79aa99ddb42569ddf97da23174d2d",
       "value": 231508.0
      }
     },
     "2035d5c7b48749209e7cfff2a8777a7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2757654236da4219a796556eaf7f2aaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_757aee1f4ef14b50ac523f5c55603398",
       "max": 433.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_73c9f4eab60942c5a9833bba36249e51",
       "value": 433.0
      }
     },
     "3a689e910b19470787dc9ef09968da84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45b5ba1b5c16430ea12456c85812c90b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2757654236da4219a796556eaf7f2aaa",
        "IPY_MODEL_ca86e399410240e6b3d1c2c653d2dc03"
       ],
       "layout": "IPY_MODEL_2035d5c7b48749209e7cfff2a8777a7e"
      }
     },
     "5439e06ef23d45ac8613c56488a460ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73c9f4eab60942c5a9833bba36249e51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "757aee1f4ef14b50ac523f5c55603398": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8005889a8c4a482394f0ed70d34b3cca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7281477533e427fa55950e2fc26bd1a",
       "placeholder": "​",
       "style": "IPY_MODEL_e67810ea27dd48fe806c0982abcb46de",
       "value": " 232k/232k [00:00&lt;00:00, 909kB/s]"
      }
     },
     "8143bada933c403794746ee98be78cbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8fa1bb743c0d497591c4ac9a270f5f9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "90b79aa99ddb42569ddf97da23174d2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "ca46eb8edaca4c23b995acd20b361703": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_013a28c3c6f24892a529d8e14ce0790f",
        "IPY_MODEL_8005889a8c4a482394f0ed70d34b3cca"
       ],
       "layout": "IPY_MODEL_3a689e910b19470787dc9ef09968da84"
      }
     },
     "ca86e399410240e6b3d1c2c653d2dc03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8143bada933c403794746ee98be78cbc",
       "placeholder": "​",
       "style": "IPY_MODEL_8fa1bb743c0d497591c4ac9a270f5f9b",
       "value": " 433/433 [00:01&lt;00:00, 331B/s]"
      }
     },
     "e67810ea27dd48fe806c0982abcb46de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7281477533e427fa55950e2fc26bd1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
